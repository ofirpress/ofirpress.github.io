---
layout: post
title: The Bamboogle Dataset
mathjax: true
---
<iframe width="560" height="315" display=block src="https://www.youtube.com/embed/Y0lwmimnAbk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<br>Bamboogle is a dataset that we constructed, made up only of questions that Google answers *incorrectly*. The leaderboard for it is [here](https://paperswithcode.com/sota/question-answering-on-bamboogle). 

In our [Compositionality Gap paper](https://arxiv.org/abs/2210.03350), we show that language models also struggle with these questions and that our self-ask prompting method substantially improves the ability of language models to answer these questions (better than Chain-of-Thought).

For move details, check out the video above. 

Bamboogle was introduced in our Compositionality Gap paper which can be found [here](https://arxiv.org/abs/2210.03350), and the dataset itself is [here](https://github.com/ofirpress/self-ask/blob/main/datasets/bamboogle.md). 
